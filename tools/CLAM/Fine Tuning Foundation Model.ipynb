{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny99457/anaconda3/envs/clam/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/johnny99457/anaconda3/envs/clam/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/data1/johnny99457/Pipeline-Processing-TCGA-Slides-for-MIL/tools/CLAM/models/conch/factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[surv label] at patient level\n",
      "\tmin/avg/median/max time = 0.0/896.69/658.0/7248.0\n",
      "\tratio of event = 0.35933806146572106\n",
      "[surv label] to continuous\n",
      "Loaded 486 slides with survival data\n"
     ]
    }
   ],
   "source": [
    "# 導入必要的庫\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from models.conch import create_model_from_pretrained\n",
    "from datasets.dataset_h5 import Whole_Slide_Bag_FP\n",
    "from utils.utils import print_network, collate_features\n",
    "from utils.file_utils import save_hdf5\n",
    "from PIL import Image\n",
    "import openslide\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SurvLabelTransformer(object):\n",
    "    \"\"\"\n",
    "    SurvLabelTransformer: create label of survival data for model training.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_label, column_t='t', column_e='e', verbose=True):\n",
    "        super(SurvLabelTransformer, self).__init__()\n",
    "        self.path_label = path_label\n",
    "        self.column_t = column_t\n",
    "        self.column_e = column_e\n",
    "        self.column_label = None\n",
    "        self.full_data = pd.read_csv(path_label, dtype={'patient_id': str, 'pathology_id': str})\n",
    "        \n",
    "        self.pat_data = self.to_patient_data(self.full_data, at_column='patient_id')\n",
    "        self.min_t = self.pat_data[column_t].min()\n",
    "        self.max_t = self.pat_data[column_t].max()\n",
    "        if verbose:\n",
    "            print('[surv label] at patient level')\n",
    "            print('\\tmin/avg/median/max time = {}/{:.2f}/{}/{}'.format(self.min_t, \n",
    "                self.pat_data[column_t].mean(), self.pat_data[column_t].median(), self.max_t))\n",
    "            print('\\tratio of event = {}'.format(self.pat_data[column_e].sum() / len(self.pat_data)))\n",
    "\n",
    "    def to_patient_data(self, df, at_column='patient_id'):\n",
    "        df_gps = df.groupby('patient_id').groups\n",
    "        df_idx = [i[0] for i in df_gps.values()]\n",
    "        return df.loc[df_idx, :]\n",
    "\n",
    "    def to_continuous(self, column_label='y'):\n",
    "        print('[surv label] to continuous')\n",
    "        self.column_label = [column_label]\n",
    "\n",
    "        label = []\n",
    "        for i in self.pat_data.index:\n",
    "            if self.pat_data.loc[i, self.column_e] == 0:\n",
    "                label.append(-1 * self.pat_data.loc[i, self.column_t])\n",
    "            else:\n",
    "                label.append(self.pat_data.loc[i, self.column_t])\n",
    "        self.pat_data.loc[:, column_label] = label\n",
    "        \n",
    "        return self.pat_data\n",
    "\n",
    "    def to_discrete(self, bins=4, column_label_t='y_t', column_label_c='y_c'):\n",
    "        \"\"\"\n",
    "        based on the quartiles of survival time values (in months) of uncensored patients.\n",
    "        see Chen et al. Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images\n",
    "        \"\"\"\n",
    "        print('[surv label] to discrete, bins = {}'.format(bins))\n",
    "        self.column_label = [column_label_t, column_label_c]\n",
    "\n",
    "        # c = 1 -> censored/no event, c = 0 -> uncensored/event\n",
    "        self.pat_data.loc[:, column_label_c] = 1 - self.pat_data.loc[:, self.column_e]\n",
    "\n",
    "        # discrete time labels\n",
    "        df_events = self.pat_data[self.pat_data[self.column_e] == 1]\n",
    "        _, qbins = pd.qcut(df_events[self.column_t], q=bins, retbins=True, labels=False)\n",
    "        qbins[0] = self.min_t - 1e-5\n",
    "        qbins[-1] = self.max_t + 1e-5\n",
    "\n",
    "        discrete_labels, qbins = pd.cut(self.pat_data[self.column_t], bins=qbins, retbins=True, labels=False, right=False, include_lowest=True)\n",
    "        self.pat_data.loc[:, column_label_t] = discrete_labels.values.astype(int)\n",
    "\n",
    "        return self.pat_data\n",
    "\n",
    "    def collect_slide_info(self, pids, column_label=None):\n",
    "        if column_label is None:\n",
    "            column_label = self.column_label\n",
    "\n",
    "        sel_pids, pid2sids, pid2label = list(), dict(), dict()\n",
    "        for pid in pids:\n",
    "            sel_idxs = self.full_data[self.full_data['patient_id'] == pid].index\n",
    "            if len(sel_idxs) > 0:\n",
    "                sel_pids.append(pid)\n",
    "                pid2sids[pid] = list(self.full_data.loc[sel_idxs, 'pathology_id'])\n",
    "                \n",
    "                pat_idx = self.pat_data[self.pat_data['patient_id'] == pid].index[0]\n",
    "                pid2label[pid] = list(self.pat_data.loc[pat_idx, column_label])\n",
    "\n",
    "            else:\n",
    "                print('[warning] patient {} not found!'.format(pid))\n",
    "\n",
    "        return sel_pids, pid2sids, pid2label\n",
    "\n",
    "# 定義WSI數據集\n",
    "class WSIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, h5_dir, slide_dir, slide_ext='.svs', custom_transforms=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.h5_dir = h5_dir\n",
    "        self.slide_dir = slide_dir\n",
    "        self.slide_ext = slide_ext\n",
    "        self.custom_transforms = custom_transforms\n",
    "        self.slide_data = self.load_slide_data()\n",
    "\n",
    "    def load_slide_data(self):\n",
    "        \"\"\"Load slide data and survival information from CSV using SurvLabelTransformer\"\"\"\n",
    "        # 初始化 SurvLabelTransformer\n",
    "        surv_label = SurvLabelTransformer(self.csv_path, verbose=True)\n",
    "        \n",
    "        # 轉換為連續的生存標籤\n",
    "        patient_data = surv_label.to_continuous(column_label='y')\n",
    "        \n",
    "        # 收集全部病理切片的信息\n",
    "        full_data = pd.read_csv(self.csv_path, dtype={'patient_id': str, 'pathology_id': str})\n",
    "        \n",
    "        # 準備返回的數據\n",
    "        slide_data = []\n",
    "        for _, row in full_data.iterrows():\n",
    "            pathology_id = row['pathology_id']\n",
    "            patient_id = row['patient_id']\n",
    "            \n",
    "            # 獲取對應病人的標籤\n",
    "            pat_idx = patient_data[patient_data['patient_id'] == patient_id].index[0]\n",
    "            label = patient_data.loc[pat_idx, 'y']\n",
    "            \n",
    "            slide_data.append((pathology_id, label))\n",
    "            \n",
    "        print(f\"Loaded {len(slide_data)} slides with survival data\")\n",
    "        return slide_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slide_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id, label = self.slide_data[idx]\n",
    "        h5_path = os.path.join(self.h5_dir, 'patches', f\"{slide_id}.h5\")\n",
    "        slide_path = os.path.join(self.slide_dir, f\"{slide_id}{self.slide_ext}\")\n",
    "        \n",
    "        # 使用Whole_Slide_Bag_FP加載WSI\n",
    "        wsi = openslide.open_slide(slide_path)\n",
    "        wsi_dataset = Whole_Slide_Bag_FP(file_path=h5_path, wsi=wsi, custom_transforms=self.custom_transforms)\n",
    "\n",
    "        # 確保數據格式正確\n",
    "        if len(wsi_dataset) > 0:\n",
    "            first_item = wsi_dataset[0]\n",
    "            if not isinstance(first_item[0], torch.Tensor):\n",
    "                raise ValueError(f\"Expected torch.Tensor, got {type(first_item[0])}\")\n",
    "            print(\"Sample tensor shape:\", first_item[0].shape)\n",
    "    \n",
    "        \n",
    "        return wsi_dataset, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# 加載預訓練模型\n",
    "def load_pretrained_model(ckpt_path, target_patch_size):\n",
    "    model, preprocess = create_model_from_pretrained(\n",
    "        \"conch_ViT-B-16\", \n",
    "        checkpoint_path=ckpt_path,\n",
    "        force_image_size=target_patch_size,\n",
    "    )\n",
    "    return model, preprocess\n",
    "\n",
    "\"\"\"\n",
    "Attention Network with Sigmoid Gating (3 fc layers)\n",
    "args:\n",
    "    L: input feature dimension\n",
    "    D: hidden layer dimension\n",
    "    dropout: whether to use dropout (p = 0.25)\n",
    "    n_classes: number of classes \n",
    "\"\"\"\n",
    "class Attn_Net_Gated(nn.Module):\n",
    "    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n",
    "        super(Attn_Net_Gated, self).__init__()\n",
    "        self.attention_a = [\n",
    "            nn.Linear(L, D),\n",
    "            nn.Tanh()]\n",
    "        \n",
    "        self.attention_b = [nn.Linear(L, D),\n",
    "                            nn.Sigmoid()]\n",
    "        if dropout:\n",
    "            self.attention_a.append(nn.Dropout(0.25))\n",
    "            self.attention_b.append(nn.Dropout(0.25))\n",
    "\n",
    "        self.attention_a = nn.Sequential(*self.attention_a)\n",
    "        self.attention_b = nn.Sequential(*self.attention_b)\n",
    "        \n",
    "        self.attention_c = nn.Linear(D, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.attention_a(x)\n",
    "        b = self.attention_b(x)\n",
    "        A = a.mul(b)\n",
    "        A = self.attention_c(A)  # N x n_classes\n",
    "        return A, x\n",
    "\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, dropout=True, dims=[512,256,128,64,32], **kwargs):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        fc = [nn.Linear(dims[0], dims[1]), \n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.25),\n",
    "              nn.Linear(dims[1], dims[2]), \n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.25),\n",
    "              nn.Linear(dims[2], dims[3]), \n",
    "        ]\n",
    "        \n",
    "        attention_net = Attn_Net_Gated(L=dims[3], D=dims[3], dropout=dropout, n_classes=1)\n",
    "        fc.append(attention_net)\n",
    "        self.attention_net = nn.Sequential(*fc)\n",
    "        self.out_layer = nn.Sequential(\n",
    "                    nn.Linear(dims[3], dims[4]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.25),\n",
    "                    nn.Linear(dims[4], 1)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_positive_targets(length, device):\n",
    "        return torch.full((length,), 1, device=device).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def create_negative_targets(length, device):\n",
    "        return torch.full((length,), 0, device=device).long()\n",
    "\n",
    "    def forward(self, h, attention_only=False):\n",
    "        h=h.squeeze(0)\n",
    "        A, h = self.attention_net(h)  # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        if attention_only:\n",
    "            return A\n",
    "        \n",
    "        A_raw = A\n",
    "        A = F.softmax(A, dim=1)  # softmax over N\n",
    "\n",
    "        M = torch.mm(A, h)  # A: 1 * N h: N * 512 => M: 1 * 512\n",
    "        # M = torch.cat([M, embed_batch], axis=1)\n",
    "        risk_score = self.out_layer(M)  \n",
    "        # Y_hat = torch.topk(risk_score, 1, dim=1)[1]\n",
    "        result = {\n",
    "            'risk_score': risk_score,\n",
    "            'attention_raw': A_raw,\n",
    "            'M': M\n",
    "        }\n",
    "        return risk_score\n",
    "\n",
    "# 定義損失函數 (Cox部分似然損失)\n",
    "def cox_loss(risk_scores, labels, device):\n",
    "    \"\"\"A partial likelihood estimation (called Breslow estimation) function in Survival Analysis.\n",
    "\n",
    "    This is a pytorch implementation by Huang. See more in https://github.com/huangzhii/SALMON.\n",
    "    Note that it only suppurts survival data with no ties (i.e., event occurrence at same time).\n",
    "    \n",
    "    Args:\n",
    "        y (Tensor): The absolute value of y indicates the last observed time. The sign of y \n",
    "        represents the censor status. Negative value indicates a censored example.\n",
    "        y_hat (Tensor): Predictions given by the survival prediction model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(cox_loss, self).__init__()\n",
    "        print('[setup] loss: a popular PLE loss in coxph')\n",
    "\n",
    "    def forward(self, y, y_hat):\n",
    "        T = torch.abs(y)\n",
    "        E = (y > 0).int()\n",
    "\n",
    "        n_batch = len(T)\n",
    "        R_matrix_train = torch.zeros([n_batch, n_batch], dtype=torch.int8)\n",
    "        for i in range(n_batch):\n",
    "            for j in range(n_batch):\n",
    "                R_matrix_train[i, j] = T[j] >= T[i]\n",
    "\n",
    "        train_R = R_matrix_train.float().to(device)\n",
    "        train_ystatus = E.float().to(device)\n",
    "\n",
    "        theta = y_hat.reshape(-1)\n",
    "        exp_theta = torch.exp(theta)\n",
    "\n",
    "        loss_nn = - torch.mean((theta - torch.log(torch.sum(exp_theta * train_R, dim=1))) * train_ystatus)\n",
    "\n",
    "        return loss_nn\n",
    "\n",
    "def extract_features(model, wsi_dataset):\n",
    "    \"\"\"\n",
    "    從單個WSI數據集中提取視覺特徵\n",
    "    Args:\n",
    "        model: CONCH model\n",
    "        wsi_dataset: 單個WSI數據集\n",
    "    Returns:\n",
    "        視覺特徵張量\n",
    "    \"\"\"\n",
    "    # 創建自定義的collate函數來處理numpy數組和張量的混合\n",
    "    def custom_collate(batch):\n",
    "        imgs = torch.stack([item[0] for item in batch])  # 使用stack替代cat\n",
    "        coords = np.vstack([item[1] for item in batch])\n",
    "        return imgs, coords\n",
    "\n",
    "    loader = DataLoader(\n",
    "        wsi_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    features = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "\n",
    "            # 處理形狀問題\n",
    "            if len(imgs.shape) == 5:  # [batch, 1, channel, height, width]\n",
    "                imgs = imgs.squeeze(1)  # 移除多餘的維度，變成 [batch, channel, height, width]\n",
    "            \n",
    "            # 確保輸入張量的形狀正確 [batch_size, channels, height, width]\n",
    "            if len(imgs.shape) != 4:\n",
    "                raise ValueError(f\"Expected 4D input tensor, got shape {imgs.shape}\")\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            actual_model = model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model\n",
    "\n",
    "             # 檢查並打印輸入形狀\n",
    "            print(\"Input shape:\", imgs.shape)\n",
    "\n",
    "            # 使用視覺編碼器提取特徵\n",
    "            output = actual_model.visual(imgs)  # 使用visual而不是forward_no_head\n",
    "            \n",
    "            # 如果輸出是tuple，取第一個元素（通常是特徵）\n",
    "            if isinstance(output, tuple):\n",
    "                vis_features = output[0]\n",
    "            else:\n",
    "                vis_features = output\n",
    "                \n",
    "            features.append(vis_features.cpu())\n",
    "    \n",
    "    return torch.cat(features, dim=0)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, accumulation_steps=5, save_dir='checkpoints'):\n",
    "\n",
    "     # 創建保存目錄\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 配置 LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        r=8,  # LoRA rank\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"qkv\", \"proj\"],  # 需要根據你的模型架構調整\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    \n",
    "    # 將模型轉換為 LoRA 模型\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print_network(model) \n",
    "\n",
    "    feature_dim = 512\n",
    "    survival_model = SurvivalModel(dims=[feature_dim, 256, 128, 64, 32]).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(survival_model.parameters()), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        survival_model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_risk_scores = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # 直接遍歷數據集\n",
    "        for i in range(len(train_loader.dataset)):\n",
    "            wsi_dataset, label = train_loader.dataset[i]\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_features(model, wsi_dataset)\n",
    "            features = features.to(device)\n",
    "            \n",
    "            # Calculate risk scores\n",
    "            risk_score = survival_model(features)\n",
    "            \n",
    "            all_risk_scores.append(risk_score)\n",
    "            all_labels.append(label)\n",
    "            \n",
    "            # 每accumulation_steps個樣本更新一次\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader.dataset):\n",
    "                # Stack collected tensors\n",
    "                y = torch.stack(all_labels).to(device)\n",
    "                y_hat = torch.cat(all_risk_scores)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = cox_loss(y_hat, y, device) / accumulation_steps\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader.dataset)}], Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Reset collectors\n",
    "                all_risk_scores = []\n",
    "                all_labels = []\n",
    "\n",
    "        # 驗證階段\n",
    "        model.eval()\n",
    "        survival_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_loader.dataset)):\n",
    "                wsi_dataset, label = val_loader.dataset[i]\n",
    "                features = extract_features(model, wsi_dataset)\n",
    "                features = features.to(device)\n",
    "                label = label.to(device)\n",
    "                \n",
    "                risk_score = survival_model(features)\n",
    "                loss = cox_loss(risk_score, label, device)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "            # 保存 LoRA 模型\n",
    "            model.save_pretrained(os.path.join(save_dir, f'best_lora_model'))\n",
    "            \n",
    "            # 保存 survival model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': survival_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, os.path.join(save_dir, f'best_survival_model.pth'))\n",
    "            \n",
    "            print(f\"Saved best model with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # 定期保存檢查點\n",
    "        if (epoch + 1) % 5 == 0:  # 每5個epoch保存一次\n",
    "            model.save_pretrained(os.path.join(save_dir, f'lora_checkpoint_epoch_{epoch+1}'))\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': survival_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, os.path.join(save_dir, f'survival_checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    return model, survival_model\n",
    "    \n",
    "# 設置參數\n",
    "csv_path = \"/data1/johnny99457/DSCA/data_split/tcga_luad_merged/tcga_luad_merged_path_full.csv\"\n",
    "h5_dir = \"/data1/johnny99457/PATCHES/LUAD/tiles-5x-s448\"\n",
    "slide_dir = \"/data1/johnny99457/DATASETS/TCGA/LUAD\"\n",
    "ckpt_path = \"/data1/johnny99457/CLAM/checkpoints/conch/pytorch_model.bin\"\n",
    "target_patch_size = 224\n",
    "batch_size = 1  # 每個batch只包含一個WSI\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 加載預訓練模型\n",
    "model, preprocess = load_pretrained_model(ckpt_path, target_patch_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# 創建數據集和數據載入器\n",
    "dataset = WSIDataset(csv_path, h5_dir, slide_dir, custom_transforms=preprocess)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 使用最簡單的DataLoader設置\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): CoCa(\n",
      "      (text): TextTransformer(\n",
      "        (token_embedding): Embedding(32007, 768)\n",
      "        (transformer): Transformer(\n",
      "          (resblocks): ModuleList(\n",
      "            (0-11): 12 x ResidualAttentionBlock(\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (ls_1): Identity()\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (gelu): GELU(approximate='none')\n",
      "                (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (ls_2): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (visual): VisualModel(\n",
      "        (trunk): VisionTransformer(\n",
      "          (patch_embed): PatchEmbed(\n",
      "            (proj): lora.Conv2d(\n",
      "              (base_layer): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Conv2d(3, 8, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Conv2d(8, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (norm): Identity()\n",
      "          )\n",
      "          (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "          (patch_drop): Identity()\n",
      "          (norm_pre): Identity()\n",
      "          (blocks): Sequential(\n",
      "            (0): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (1): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (2): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (3): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (4): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (5): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (6): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (7): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (8): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (9): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (10): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (11): Block(\n",
      "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU(approximate='none')\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc_norm): Identity()\n",
      "          (head_drop): Dropout(p=0.0, inplace=False)\n",
      "          (head): Identity()\n",
      "        )\n",
      "        (attn_pool_contrast): AttentionalPooler(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (ln_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ln_contrast): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (head): Sequential()\n",
      "        (attn_pool_caption): AttentionalPooler(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ln_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ln_caption): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (text_decoder): MultimodalTransformer(\n",
      "        (resblocks): ModuleList(\n",
      "          (0-11): 12 x ResidualAttentionBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ls_1): Identity()\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): GELU(approximate='none')\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ls_2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (cross_attn): ModuleList(\n",
      "          (0-11): 12 x ResidualAttentionBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ls_1): Identity()\n",
      "            (ln_1_kv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): GELU(approximate='none')\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ls_2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 395235841\n",
      "Total number of trainable parameters: 454656\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41247 37770]\n",
      "level_dim [41247 37770]\n",
      "name TCGA-55-8087-01Z-00-DX1.548f2800-8caf-4c0e-a7b5-6d3d28315d9c\n",
      "patch_level 0\n",
      "patch_size 3584\n",
      "save_path /work/u6658716/TCGA-LUAD/CLAM/PATCHES/New_LUAD/tiles-5x-s448/patches\n",
      "\n",
      "feature extraction settings:\n",
      "-- target patch size:  None\n",
      "-- imagenet_pretrained:  False\n",
      "-- patches sampler: None\n",
      "-- color normalization: None\n",
      "-- color argmentation: None\n",
      "-- add_patch_noise: None\n",
      "-- vertical_flip: False\n",
      "-- transformations:  Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x74f8e55714c0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "Sample tensor shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([17, 3, 224, 224])\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47962 31231]\n",
      "level_dim [47962 31231]\n",
      "name TCGA-55-8511-01Z-00-DX1.8EDFB05B-5B59-46EA-973C-1048B1E284D2\n",
      "patch_level 0\n",
      "patch_size 3584\n",
      "save_path /work/u6658716/TCGA-LUAD/CLAM/PATCHES/New_LUAD/tiles-5x-s448/patches\n",
      "\n",
      "feature extraction settings:\n",
      "-- target patch size:  None\n",
      "-- imagenet_pretrained:  False\n",
      "-- patches sampler: None\n",
      "-- color normalization: None\n",
      "-- color argmentation: None\n",
      "-- add_patch_noise: None\n",
      "-- vertical_flip: False\n",
      "-- transformations:  Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x74f8e55714c0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "Sample tensor shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([30, 3, 224, 224])\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [70024 30904]\n",
      "level_dim [70024 30904]\n",
      "name TCGA-99-8028-01Z-00-DX1.23de89b1-67f8-41fb-980a-010ea190d687\n",
      "patch_level 0\n",
      "patch_size 3584\n",
      "save_path /work/u6658716/TCGA-LUAD/CLAM/PATCHES/New_LUAD/tiles-5x-s448/patches\n",
      "\n",
      "feature extraction settings:\n",
      "-- target patch size:  None\n",
      "-- imagenet_pretrained:  False\n",
      "-- patches sampler: None\n",
      "-- color normalization: None\n",
      "-- color argmentation: None\n",
      "-- add_patch_noise: None\n",
      "-- vertical_flip: False\n",
      "-- transformations:  Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x74f8e55714c0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "Sample tensor shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([17, 3, 224, 224])\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104557  64154]\n",
      "level_dim [104557  64154]\n",
      "name TCGA-86-8669-01Z-00-DX1.845b8b4e-6445-4da1-8275-42b765c024cc\n",
      "patch_level 0\n",
      "patch_size 3584\n",
      "save_path /work/u6658716/TCGA-LUAD/CLAM/PATCHES/New_LUAD/tiles-5x-s448/patches\n",
      "\n",
      "feature extraction settings:\n",
      "-- target patch size:  None\n",
      "-- imagenet_pretrained:  False\n",
      "-- patches sampler: None\n",
      "-- color normalization: None\n",
      "-- color argmentation: None\n",
      "-- add_patch_noise: None\n",
      "-- vertical_flip: False\n",
      "-- transformations:  Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x74f8e55714c0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "Sample tensor shape: torch.Size([1, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Input shape: torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 訓練模型\n",
    "train_model(model, train_loader, val_loader, num_epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
